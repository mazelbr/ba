{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matze\\Documents\\Winton\\CRISPDM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "print(os.getcwd()) \n",
    "\n",
    "X = pd.read_csv('X_features.csv', index_col=0)\n",
    "y = pd.read_csv('y_features.csv', index_col=0)\n",
    "groups = X.pop('Feature_7')\n",
    "\n",
    "test = pd.read_csv('test_features.csv', index_col=0).drop(columns = 'Feature_7')\n",
    "sample = pd.read_csv('sample_submission_2.csv')\n",
    "df_train = pd.read_csv('train.csv').set_index('Id')\n",
    "df_test = pd.read_csv('test_2.csv').set_index('Id')\n",
    "weights = df_train.loc[:,'Weight_Intraday':'Weight_Daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "def typecasting(df):\n",
    "    categorial_cols = ['Feature_20','Feature_5']\n",
    "    binary_cols = ['Feature_16']\n",
    "\n",
    "    for col in categorial_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    \n",
    "    for col in binary_cols:\n",
    "        df[col] = (df[col]-1).astype('category')\n",
    "\n",
    "def trainscore(y_true,y_pred,weights):\n",
    "    weights = weights.loc[y_true.index,:]\n",
    "    daily = pd.concat([weights.Weight_Daily]*2,axis=1)\n",
    "    minute = pd.concat([weights.Weight_Intraday]*60,axis=1)\n",
    "    weights = pd.concat([minute,daily],axis=1)\n",
    "    weights.columns = df_train.loc[:,'Ret_121':'Ret_PlusTwo'].columns\n",
    "    \n",
    "    \n",
    "    wmea = (((abs(y_true-y_pred))*weights).values.flatten().sum()/y_true.size)\n",
    "    \n",
    "    \n",
    "    #minute_ret = abs(y_true.loc[:,'Ret_121':'180']-y_pred.loc[:,'Ret_121':'Ret_180'])*weights.Weight_Intraday\n",
    "    return wmea\n",
    "\n",
    "\n",
    "def create_core_features(train,test):\n",
    "    X_train=train.loc[:,'Feature_1':'Ret_MinusOne']\n",
    "    X_val=test.loc[:,'Feature_1':'Ret_MinusOne']\n",
    "    from sklearn.preprocessing import Imputer\n",
    "\n",
    "    typecasting(X_train)\n",
    "    X_train = pd.get_dummies(X_train,drop_first=True)\n",
    "\n",
    "    typecasting(X_val)\n",
    "    X_val = pd.get_dummies(X_val,drop_first=True)\n",
    "\n",
    "    imputer = Imputer(strategy='mean', axis=0)\n",
    "    X_train = pd.DataFrame(imputer.fit_transform(X_train), index= X_train.index,columns=X_train.columns)\n",
    "    X_val = pd.DataFrame(imputer.fit_transform(X_val), index= X_val.index,columns=X_val.columns)\n",
    "    X_train['Ret_MinutePast'] = train.loc[:,'Ret_2':'Ret_120'].sum(axis=1)\n",
    "    X_val['Ret_MinutePast'] = test.loc[:,'Ret_2':'Ret_120'].sum(axis=1)\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_features, core_features_test = create_core_features(df_train,df_test)\n",
    "# generated features is X\n",
    "#Adding the interactions to Featureset\n",
    "crafted_features = pd.read_csv('BA_Features/X_features_BA.csv', index_col=0)\n",
    "crafted_features_test = pd.read_csv('BA_Features/test_features_BA.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 102) (120000, 102)\n"
     ]
    }
   ],
   "source": [
    "crafted_features['interaction_3']= crafted_features.PR_120 * crafted_features.grouped_mad_fet_7\n",
    "crafted_features_test['interaction_3']= crafted_features_test.PR_120 * test.grouped_mad_fet_7\n",
    "\n",
    "#X['interaction_4']= X.smoothed_minute_mean * X.minute_mad\n",
    "#test['interaction_4']= test.smoothed_minute_mean * test.minute_mad\n",
    "\n",
    "crafted_features['interaction_5']= crafted_features.minute_mad * crafted_features.PR_120\n",
    "crafted_features_test['interaction_5']= crafted_features_test.minute_mad * crafted_features_test.PR_120\n",
    "\n",
    "crafted_features['interaction_6']= crafted_features.smoothed_minute_mean * crafted_features.grouped_mad_fet_5\n",
    "crafted_features_test['interaction_6']= crafted_features_test.smoothed_minute_mean * crafted_features_test.grouped_mad_fet_5\n",
    "\n",
    "crafted_features['interaction_7']= crafted_features.PR_120 * crafted_features.fet_7_Minute_MAD\n",
    "crafted_features_test['interaction_7']= crafted_features_test.PR_120 * crafted_features_test.fet_7_Minute_MAD\n",
    "\n",
    "crafted_features['interaction_8']= crafted_features.fet_7_Minute_MAD * crafted_features.fet_7_Minute_Mean\n",
    "crafted_features_test['interaction_8']= crafted_features_test.fet_7_Minute_MAD * crafted_features_test.fet_7_Minute_Mean\n",
    "\n",
    "print(crafted_features.shape,crafted_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.concat([core_features,crafted_features],axis=1).drop('Feature_7',axis='columns')\n",
    "all_features_test = pd.concat([core_features_test,crafted_features_test],axis=1).drop('Feature_7',axis='columns')\n",
    "\n",
    "features_and_target = all_features.copy()\n",
    "features_and_target['Ret_PlusOne'] = y.Ret_PlusOne\n",
    "features_and_target['Ret_PlusTwo'] = y.Ret_PlusTwo\n",
    "features_and_target['Ret_MinutesFut'] = df_train.loc[:,'Ret_121':'Ret_180'].sum(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 111.148 s\n"
     ]
    }
   ],
   "source": [
    "#import time\n",
    "#t0=time.time()\n",
    "#corr_spearman = features_and_target.corr(method='spearman').to_csv('BA_Features/spearman_cor.csv')\n",
    "print(\"time:\", round(time.time()-t0, 3), \"s\")\n",
    "#corr_pearson = features_and_target.corr(method='pearson').to_csv('BA_Features/pearson_cor.csv')\n",
    "#print(\"time:\", round(time.time()-t0, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "n=20000\n",
    "\n",
    "t0 = time.time()\n",
    "mic_Ret_PlusOne = mutual_info_regression(all_features.sample(n,random_state=0), y.Ret_PlusOne.sample(n,random_state=0), random_state=0)\n",
    "print('Time: ',time.time()-t0)\n",
    "t0 = time.time()\n",
    "#select10 = SelectKBest(score_func=mutual_info_regression, k=20).fit(all_features.sample(n,random_state=0), y.Ret_PlusOne.sample(n,random_state=0))\n",
    "print('Time: ',time.time()-t0)\n",
    "mic_Ret_PlusTwo = mutual_info_regression(all_features.sample(n,random_state=0), y.Ret_PlusTwo.sample(n,random_state=0), random_state=0)\n",
    "print('Time: ',time.time()-t0)\n",
    "t0 = time.time()\n",
    "mic_Ret_Minute = mutual_info_regression(all_features.sample(n,random_state=0), df_train.loc[:,'Ret_121':'Ret_180'].sum(axis=1).sample(n,random_state=0), random_state=0)\n",
    "\n",
    "print('Time: ',time.time()-t0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([mic_Ret_PlusOne,mic_Ret_PlusTwo,mic_Ret_Minute], index = ['MIC_PlusOne','MIC_PlusTwo','MIC_Minute'],columns=all_features.columns.values).T.to_csv('BA_Features/MIC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures()\n",
    "X_poly = pd.read_csv('X_features_2.csv', index_col=0)\n",
    "X_poly_test = pd.read_csv('test_features_2.csv', index_col=0)\n",
    "X_poly = pd.DataFrame(poly.fit_transform(X_poly), columns= poly.get_feature_names(),index=X_poly.index).drop(['1'],axis='columns')\n",
    "X_poly_test=pd.DataFrame(poly.fit_transform(X_poly_test), columns= poly.get_feature_names(),index=X_poly_test.index).drop(['1'],axis='columns')\n",
    "display(X_poly.head())\n",
    "display(X_poly_test.head())\n",
    "\n",
    "X_poly_and_target = X_poly.copy()\n",
    "X_poly_and_target['Ret_PlusOne'] = y.Ret_PlusOne\n",
    "X_poly_and_target['Ret_PlusTwo'] = y.Ret_PlusTwo\n",
    "X_poly_and_target['Ret_MinutesFut'] = df_train.loc[:,'Ret_121':'Ret_180'].sum(axis=1)\n",
    "\n",
    "X_poly_and_target.corr(method='spearman').to_csv('BA_Features/poly_cor.csv')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIC = pd.read_csv('BA_Features/MIC.csv', index_col=0)\n",
    "corr_spearman = pd.read_csv('BA_Features/spearman_cor.csv',index_col=0).drop(['Ret_MinutesFut','Ret_PlusOne','Ret_PlusTwo'])\n",
    "corr_poly = pd.read_csv('BA_Features/poly_cor.csv',index_col=0).drop(['Ret_MinutesFut','Ret_PlusOne','Ret_PlusTwo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIC_poly = pd.read_csv('BA_Features/MIC_Poly.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Ret_PlusOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featuresets (Needed once for each Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['algo','featureset','target','fitting_time','zerobenchmark','validationmark','mae','params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic10 = MIC.MIC_PlusOne.sort_values(ascending=False)[0:10].index\n",
    "mic20 = MIC.MIC_PlusOne.sort_values(ascending=False)[0:20].index\n",
    "mic30 = MIC.MIC_PlusOne.sort_values(ascending=False)[0:30].index\n",
    "corr10 = abs(corr_spearman.Ret_PlusOne).sort_values(ascending=False)[0:10].index\n",
    "corr20 = abs(corr_spearman.Ret_PlusOne).sort_values(ascending=False)[0:20].index\n",
    "corr30 = abs(corr_spearman.Ret_PlusOne).sort_values(ascending=False)[0:30].index\n",
    "core=core_features.drop('Feature_7',axis='columns').columns.values\n",
    "poly15_1 = ['x1 x8','x3 x16', 'x5 x10', 'x5 x16', 'x5', 'x5 x6', 'x3 x6', 'x3 x5', 'x5 x11', 'x3 x10', 'x3^2', 'x2^2', 'x3', 'x3 x12']\n",
    "poly15_2 = abs(corr_poly.Ret_PlusOne).sort_values(ascending=False)[0:15].index\n",
    "micpoly15 = MIC_poly.MIC_PlusOne.sort_values(ascending=False)[0:15].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RFE Params\n",
    "rfe_learner = Ridge()\n",
    "rfe_params ={'alpha':100,'normalize':True,'max_iter':700,'random_state':[0]}\n",
    "rfe_learner = rfe_learner.set_params(**rfe_params)\n",
    "rfe_target = 'Ret_PlusOne'\n",
    "\n",
    "#RFE Fitting\n",
    "rfe = RFE(rfe_learner,n_features_to_select=10, step=1, verbose=0)\n",
    "rfe.fit(all_features,y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe10 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "rfe = RFE(rfe_learner,n_features_to_select=20, step=1, verbose=0)\n",
    "rfe.fit(all_features, y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe20 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "rfe = RFE(rfe_learner,n_features_to_select=30, step=1, verbose=0)\n",
    "rfe.fit(all_features,y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe30 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = Ridge()\n",
    "target= 'Ret_PlusOne'\n",
    "param_grid= {'alpha': [20,19,18,17,16,15,12,11,10,8,6,4,2,1,0.5],'normalize':[True],'max_iter':[700],'random_state':[0]}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,rfe10,rfe20,rfe30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','rfe10','rfe20','rfe30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "         ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Model_Params\n",
    "\n",
    "learner = HuberRegressor()\n",
    "target= 'Ret_PlusOne'\n",
    "param_grid= {'alpha': [100,20,19,18,17,16,15,12,11,10],'max_iter':[700]}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "         ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['core', 'mic10', 'mic20', 'mic30', 'corr10', 'corr20', 'corr30',\n",
       "       'polycorr15_1', 'polycorr15_2', 'micpoly15'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'core'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'micpoly15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = ExtraTreesRegressor()\n",
    "target= 'Ret_PlusOne'\n",
    "param_grid= {'max_depth': [3],#tiefer\n",
    "             \n",
    "             'max_features': 'auto',#wahrscheinlich weniger glaub kürzere fitting time\n",
    "             'min_samples_leaf': [5000], #höher\n",
    "             #'min_samples_split': [],\n",
    "             'n_estimators': [100,150],#mehr bedeuten dauert länger\n",
    "             'n_jobs':[-1],\n",
    "            'random_state':[0],\n",
    "            'max_features':['sqrt']}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "     ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['core', 'mic10', 'mic20', 'mic30', 'corr10', 'corr20', 'corr30',\n",
       "       'rfe10', 'rfe20', 'rfe30', 'polycorr15_1', 'polycorr15_2', 'micpoly15'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'core'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1061.6622803211212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "244.98215889930725"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "462.7015519142151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "646.8382456302643"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "264.24470138549805"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "622.6458895206451"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "923.2519600391388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rfe10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "106.3701913356781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rfe20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "199.81747579574585"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rfe30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "675.3823971748352"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "400.7375810146332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "259.4381444454193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'micpoly15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "320.37823510169983"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = GradientBoostingRegressor()\n",
    "target= 'Ret_PlusOne'\n",
    "param_grid= {'max_depth': [3,5],#tiefer\n",
    "             'max_features': ['auto'],#wahrscheinlich weniger glaub kürzere fitting time\n",
    "             'min_samples_split': [4000, 5000],#kann wahrscheinlich weg\n",
    "             'n_estimators': [200, 400],\n",
    "            'random_state':[0]}#mehr bedeuten dauert länger\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "    display(fitting_time)\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "    ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ret_PlusTwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic10 = MIC.MIC_PlusTwo.sort_values(ascending=False)[0:10].index\n",
    "mic20 = MIC.MIC_PlusTwo.sort_values(ascending=False)[0:20].index\n",
    "mic30 = MIC.MIC_PlusTwo.sort_values(ascending=False)[0:30].index\n",
    "corr10 = abs(corr_spearman.Ret_PlusTwo).sort_values(ascending=False)[0:10].index\n",
    "corr20 = abs(corr_spearman.Ret_PlusTwo).sort_values(ascending=False)[0:20].index\n",
    "corr30 = abs(corr_spearman.Ret_PlusTwo).sort_values(ascending=False)[0:30].index\n",
    "#corrPear = abs(corr_pearson.Ret_PlusTwo).sort_values(ascending=False)[0:20].index\n",
    "core=core_features.drop('Feature_7',axis='columns').columns.values\n",
    "poly15_1 = ['x1 x8','x3 x16', 'x5 x10', 'x5 x16', 'x5', 'x5 x6', 'x3 x6', 'x3 x5', 'x5 x11', 'x3 x10', 'x3^2', 'x2^2', 'x3', 'x3 x12']\n",
    "poly15_2 = poly15_2 = abs(corr_poly.Ret_PlusTwo).sort_values(ascending=False)[0:15].index\n",
    "micpoly15 = MIC_poly.MIC_PlusOne.sort_values(ascending=False)[0:15].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE Params\n",
    "rfe_learner = Ridge()\n",
    "rfe_params ={'alpha':100,'normalize':True,'max_iter':700,'random_state':[0]}\n",
    "rfe_learner = rfe_learner.set_params(**rfe_params)\n",
    "rfe_target = 'Ret_PlusTwo'\n",
    "\n",
    "#RFE Fitting\n",
    "rfe = RFE(rfe_learner,n_features_to_select=10, step=1, verbose=1)\n",
    "rfe.fit(all_features,y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe10 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "rfe = RFE(rfe_learner,n_features_to_select=20, step=1, verbose=1)\n",
    "rfe.fit(all_features, y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe20 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "rfe = RFE(rfe_learner,n_features_to_select=30, step=1, verbose=0)\n",
    "rfe.fit(all_features,y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe30 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = Ridge()\n",
    "target= 'Ret_PlusTwo'\n",
    "param_grid= {'alpha': [20,19,18,17,16,15,12,11,10,8,6,4,2,1,0.5],'normalize':[True],'max_iter':[700],'random_state':[0]}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,rfe10,rfe20,rfe30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','rfe10','rfe20','rfe30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "         ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['core', 'mic10', 'mic20', 'mic30', 'corr10', 'corr20', 'corr30',\n",
       "       'rfe10', 'rfe20', 'rfe30', 'polycorr15_1', 'polycorr15_2', 'micpoly15'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'micpoly15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RFE Params\n",
    "rfe_learner = HuberRegressor()\n",
    "rfe_params ={'alpha':100,'max_iter':700}\n",
    "rfe_learner = rfe_learner.set_params(**rfe_params)\n",
    "rfe_target = 'Ret_PlusTwo'\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = HuberRegressor()\n",
    "target= 'Ret_PlusTwo'\n",
    "param_grid= {'alpha': [100,20,19,18,17,16,15,12,11],'epsilon':[1.35],'max_iter':[700]}\n",
    "\n",
    "results = pd.DataFrame(columns=['algo','featureset','target','fitting_time','zerobenchmark','validationmark','mae','params'])\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "         ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = ExtraTreesRegressor()\n",
    "target= 'Ret_PlusOne'\n",
    "param_grid= {'max_depth': [3],#tiefer\n",
    "             \n",
    "             'max_features': 'auto',#wahrscheinlich weniger glaub kürzere fitting time\n",
    "             'min_samples_leaf': [5000], #höher\n",
    "             #'min_samples_split': [],\n",
    "             'n_estimators': [100,150],#mehr bedeuten dauert länger\n",
    "             'n_jobs':[-1],\n",
    "            'random_state':[0],\n",
    "            'max_features':['sqrt']}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    grid[target] = ((gs.predict(train[cols_to_use])))\n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    grid[target] = ((gs.predict(test[cols_to_use])))\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "     ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ret_Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['Ret_MinutesFut'] = df_train.loc[:, 'Ret_121':'Ret_180'].sum(axis=1)\n",
    "y['Ret_MinutesFut_20'] = df_train.loc[:, 'Ret_121':'Ret_130'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MIC_PlusOne</th>\n",
       "      <th>MIC_PlusTwo</th>\n",
       "      <th>MIC_Minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x0</th>\n",
       "      <td>0.061024</td>\n",
       "      <td>0.070398</td>\n",
       "      <td>0.089123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.065661</td>\n",
       "      <td>0.069089</td>\n",
       "      <td>0.081241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>0.037624</td>\n",
       "      <td>0.034081</td>\n",
       "      <td>0.048443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.041692</td>\n",
       "      <td>0.031685</td>\n",
       "      <td>0.045471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>0.043810</td>\n",
       "      <td>0.042430</td>\n",
       "      <td>0.063234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MIC_PlusOne  MIC_PlusTwo  MIC_Minute\n",
       "x0     0.061024     0.070398    0.089123\n",
       "x1     0.065661     0.069089    0.081241\n",
       "x2     0.037624     0.034081    0.048443\n",
       "x3     0.041692     0.031685    0.045471\n",
       "x4     0.043810     0.042430    0.063234"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIC_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic10 = MIC.MIC_Minute.sort_values(ascending=False)[0:10].index\n",
    "mic20 = MIC.MIC_Minute.sort_values(ascending=False)[0:20].index\n",
    "mic30 = MIC.MIC_Minute.sort_values(ascending=False)[0:30].index\n",
    "corr10 = abs(corr_spearman.Ret_MinutesFut).sort_values(ascending=False)[0:10].index\n",
    "corr20 = abs(corr_spearman.Ret_MinutesFut).sort_values(ascending=False)[0:20].index\n",
    "corr30 = abs(corr_spearman.Ret_MinutesFut).sort_values(ascending=False)[0:30].index\n",
    "#corrPear = abs(corr_pearson.Ret_PlusTwo).sort_values(ascending=False)[0:20].index\n",
    "core=core_features.drop('Feature_7',axis='columns').columns.values\n",
    "poly15_1 = ['x1 x8','x3 x16', 'x5 x10', 'x5 x16', 'x5', 'x5 x6', 'x3 x6', 'x3 x5', 'x5 x11', 'x3 x10', 'x3^2', 'x2^2', 'x3', 'x3 x12']\n",
    "poly15_2 = poly15_2 = abs(corr_poly.Ret_MinutesFut).sort_values(ascending=False)[0:15].index\n",
    "micpoly15 = MIC_poly.MIC_Minute.sort_values(ascending=False)[0:15].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "training time: 75.771 s\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "training time: 89.824 s\n",
      "training time: 103.391 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['core', 'mic10', 'mic20', 'mic30', 'corr10', 'corr20', 'corr30',\n",
       "       'rfe10', 'rfe20', 'rfe30', 'polycorr15_1', 'polycorr15_2', 'micpoly15'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'core'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rfe10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rfe20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rfe30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'micpoly15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#RFE Params\n",
    "rfe_learner = Ridge()\n",
    "rfe_params ={'alpha':100,'normalize':True,'max_iter':700,'random_state':0}\n",
    "rfe_learner = rfe_learner.set_params(**rfe_params)\n",
    "rfe_target = 'Ret_MinutesFut'\n",
    "\n",
    "#RFE Fitting\n",
    "rfe = RFE(rfe_learner,n_features_to_select=10, step=1, verbose=0)\n",
    "rfe.fit(all_features,y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe10 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "rfe = RFE(rfe_learner,n_features_to_select=20, step=1, verbose=0)\n",
    "rfe.fit(all_features, y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe20 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "rfe = RFE(rfe_learner,n_features_to_select=30, step=1, verbose=0)\n",
    "rfe.fit(all_features,y[rfe_target])\n",
    "print(\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "rfe30 = all_features.columns[rfe.get_support(indices=True)]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = Ridge()\n",
    "target= 'Ret_MinutesFut'\n",
    "param_grid= {'alpha': [100,50,35,20,10],'normalize':[True],'max_iter':[700]}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,rfe10,rfe20,rfe30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','rfe10','rfe20','rfe30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    \n",
    "\n",
    "    for cols in grid.loc[:,'Ret_121':'Ret_180'].columns:\n",
    "        \n",
    "        grid[cols] = ((gs.predict(train[cols_to_use]))/60)\n",
    "    \n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    for cols in grid.loc[:,'Ret_121':'Ret_180'].columns:\n",
    "        grid[cols] = ((gs.predict(test[cols_to_use]))/60)\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "     ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mic10', 'mic20', 'mic30', 'corr10', 'corr20', 'corr30', 'polycorr15_1',\n",
       "       'polycorr15_2', 'micpoly15'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mic30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr20'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'corr30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'polycorr15_2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'micpoly15'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = HuberRegressor()\n",
    "target= 'Ret_MinutesFut'\n",
    "param_grid= {'alpha': [20,5],'max_iter':[700]}\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,31))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    \n",
    "\n",
    "    for cols in grid.loc[:,'Ret_121':'Ret_180'].columns:\n",
    "        \n",
    "        grid[cols] = ((gs.predict(train[cols_to_use]))/60)\n",
    "    \n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    for cols in grid.loc[:,'Ret_121':'Ret_180'].columns:\n",
    "        grid[cols] = ((gs.predict(test[cols_to_use]))/60)\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    #sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "     ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xtra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "group_shuffle = GroupShuffleSplit(n_splits=10, test_size=0.60, train_size=0.4, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "#Model_Params\n",
    "train = pd.concat([all_features,X_poly],axis=1)\n",
    "test = pd.concat([all_features_test,X_poly_test],axis=1)\n",
    "learner = ExtraTreesRegressor()\n",
    "target= 'Ret_MinutesFut'\n",
    "param_grid= {'max_depth': [3],#tiefer\n",
    "             \n",
    "             'max_features': 'auto',#wahrscheinlich weniger glaub kürzere fitting time\n",
    "             'min_samples_leaf': [5000], #höher\n",
    "             #'min_samples_split': [],\n",
    "             'n_estimators': [100,150],#mehr bedeuten dauert länger\n",
    "             'n_jobs':[-1],\n",
    "            'random_state':[0],\n",
    "            'max_features':['sqrt']}\n",
    "\n",
    "\n",
    "\n",
    "#Choose Featuresets\n",
    "featuresets = pd.DataFrame([core,mic10,mic20,mic30,corr10,corr20,corr30,poly15_1,poly15_2,micpoly15],index=['core','mic10','mic20','mic30','corr10','corr20','corr30','polycorr15_1','polycorr15_2','micpoly15'],columns=list(range(1,43))).T\n",
    "display(featuresets.columns)\n",
    "#Fitting\n",
    "\n",
    "for name in featuresets.columns:\n",
    "\n",
    "\n",
    "    cols_to_use = featuresets[name].dropna()\n",
    "    \n",
    "    t0=time.time()\n",
    "    gs= GridSearchCV(learner, param_grid, iid=False, refit=True, cv=group_shuffle, return_train_score =True, scoring='neg_mean_absolute_error')\n",
    "    gs.fit(train[cols_to_use], y[target], groups)\n",
    "    gs.cv_results_ \n",
    "    #display(gs.best_estimator_)\n",
    "    #display(gs.best_score_ )\n",
    "    display(name)\n",
    "    fitting_time = time.time()-t0\n",
    "    #Trainscore\n",
    "    grid = pd.DataFrame(np.zeros([40000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns,index=df_train.index)\n",
    "    grid.shape\n",
    "    zerobenchmark= trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5)\n",
    "    \n",
    "\n",
    "    for cols in grid.loc[:,'Ret_121':'Ret_180'].columns:\n",
    "        \n",
    "        grid[cols] = ((gs.predict(train[cols_to_use]))/60)\n",
    "    \n",
    "    validationbenchmark = trainscore(df_train.loc[:,'Ret_121':'Ret_PlusTwo'],grid,weights).round(5) \n",
    "\n",
    "\n",
    "    ###Submission\n",
    "    grid = pd.DataFrame(np.zeros([120000, 62]), columns = df_train.loc[:, 'Ret_121':'Ret_PlusTwo'].columns)\n",
    "    grid.shape\n",
    "\n",
    "    for cols in grid.loc[:,'Ret_121':'Ret_180'].columns:\n",
    "        grid[cols] = ((gs.predict(test[cols_to_use]))/60)\n",
    "\n",
    "    sample['Predicted'] = grid.values.flatten()\n",
    "    sample.to_csv(('BA_Submissions/'+str(learner).split('(',1)[0]+'_'+name+target+\".csv\"), index=False)\n",
    "\n",
    "     ###Output\n",
    "    algo = str(learner).split('(',1)[0]\n",
    "    params= gs.best_estimator_.get_params()\n",
    "    mae = abs(gs.best_score_)\n",
    "    results = results.append(pd.DataFrame([algo,name,target,fitting_time,zerobenchmark,validationbenchmark,mae,params],index=results.columns).T, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>featureset</th>\n",
       "      <th>target</th>\n",
       "      <th>fitting_time</th>\n",
       "      <th>zerobenchmark</th>\n",
       "      <th>validationmark</th>\n",
       "      <th>mae</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>core</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>6.70325</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.13</td>\n",
       "      <td>0.00485384</td>\n",
       "      <td>{'alpha': 6, 'copy_X': True, 'fit_intercept': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mic10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>2.95934</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.12</td>\n",
       "      <td>0.00485556</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mic20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>4.24575</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.2</td>\n",
       "      <td>0.00486075</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mic30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>5.65856</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.24</td>\n",
       "      <td>0.00485748</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>corr10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>2.88769</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.11</td>\n",
       "      <td>0.00485946</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>corr20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>4.3113</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.12</td>\n",
       "      <td>0.00486077</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>corr30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>5.64229</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.12</td>\n",
       "      <td>0.00486032</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>rfe10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>2.90213</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.1</td>\n",
       "      <td>0.00485698</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>rfe20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>4.15243</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.11</td>\n",
       "      <td>0.00485746</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>rfe30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>5.97855</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.19</td>\n",
       "      <td>0.00486123</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>polycorr15_1</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>3.58621</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.08</td>\n",
       "      <td>0.00485676</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>polycorr15_2</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>3.74265</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.16</td>\n",
       "      <td>0.00486672</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>micpoly15</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>3.77019</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.15</td>\n",
       "      <td>0.00486089</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>core</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>2.43327</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.00485493</td>\n",
       "      <td>{'alpha': 20, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mic10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>0.888162</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.08</td>\n",
       "      <td>0.00485376</td>\n",
       "      <td>{'alpha': 50, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mic20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.07</td>\n",
       "      <td>0.00485433</td>\n",
       "      <td>{'alpha': 100, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>mic30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.89993</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.1</td>\n",
       "      <td>0.00485205</td>\n",
       "      <td>{'alpha': 50, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>corr10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>0.848941</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.00485665</td>\n",
       "      <td>{'alpha': 200, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>corr20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.29687</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.00485679</td>\n",
       "      <td>{'alpha': 200, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>corr30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.65108</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.00485673</td>\n",
       "      <td>{'alpha': 200, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>rfe10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>0.995591</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.00485593</td>\n",
       "      <td>{'alpha': 50, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>rfe20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.24556</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.08</td>\n",
       "      <td>0.00485495</td>\n",
       "      <td>{'alpha': 50, 'copy_X': True, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>rfe30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.6654</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.08</td>\n",
       "      <td>0.00485463</td>\n",
       "      <td>{'alpha': 100, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>polycorr15_1</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.01822</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.08</td>\n",
       "      <td>0.00485637</td>\n",
       "      <td>{'alpha': 200, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>polycorr15_2</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.07162</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.0048573</td>\n",
       "      <td>{'alpha': 200, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>micpoly15</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>1.08723</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.07</td>\n",
       "      <td>0.00485498</td>\n",
       "      <td>{'alpha': 100, 'copy_X': True, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>core</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>11.5318</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00485626</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'mse', 'max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>mic10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>9.74065</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00485601</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'mse', 'max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>mic20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>9.95673</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0048561</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'mse', 'max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>mic30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>10.1639</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00485511</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'mse', 'max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_1</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>21.3501</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.07</td>\n",
       "      <td>0.00484718</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_2</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>27.5333</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.26</td>\n",
       "      <td>0.00484339</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>micpoly15</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>14.7018</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484705</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>8.13647</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484682</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>15.8525</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.12</td>\n",
       "      <td>0.00484209</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>46.7602</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.64</td>\n",
       "      <td>0.00484169</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>28.2562</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.72</td>\n",
       "      <td>0.00483175</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>53.5698</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.78</td>\n",
       "      <td>0.00483822</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>249.03</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.83</td>\n",
       "      <td>0.0048415</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_1</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>16.0943</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.07</td>\n",
       "      <td>0.00484718</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_2</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>18.3686</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.26</td>\n",
       "      <td>0.00484339</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>micpoly15</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>8.58279</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484705</td>\n",
       "      <td>{'alpha': 200, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>3.84467</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484679</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>6.74144</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.03</td>\n",
       "      <td>0.00484251</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>16.4485</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.54</td>\n",
       "      <td>0.00483515</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>11.5186</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.67</td>\n",
       "      <td>0.00482982</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>25.312</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.74</td>\n",
       "      <td>0.00483566</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>116.644</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.79</td>\n",
       "      <td>0.00483677</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_1</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>6.9072</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.04</td>\n",
       "      <td>0.00484686</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_2</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>9.00794</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.22</td>\n",
       "      <td>0.00484556</td>\n",
       "      <td>{'alpha': 100, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>micpoly15</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>4.39591</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484688</td>\n",
       "      <td>{'alpha': 500, 'epsilon': 1.35, 'fit_intercept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>4.08746</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484721</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>8.13355</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.25</td>\n",
       "      <td>0.00484593</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>mic30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>38.9431</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.73</td>\n",
       "      <td>0.00486028</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr10</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>17.5924</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.76</td>\n",
       "      <td>0.00483674</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr20</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>35.7515</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.81</td>\n",
       "      <td>0.00484579</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>corr30</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>121.18</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.86</td>\n",
       "      <td>0.00484675</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_1</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>9.35712</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.09</td>\n",
       "      <td>0.00484932</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>polycorr15_2</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>11.0073</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1774.31</td>\n",
       "      <td>0.0048409</td>\n",
       "      <td>{'alpha': 5, 'epsilon': 1.35, 'fit_intercept':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>micpoly15</td>\n",
       "      <td>Ret_MinutesFut</td>\n",
       "      <td>4.07948</td>\n",
       "      <td>1773.92</td>\n",
       "      <td>1773.93</td>\n",
       "      <td>0.00484913</td>\n",
       "      <td>{'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    algo    featureset          target fitting_time  \\\n",
       "0                  Ridge          core  Ret_MinutesFut      6.70325   \n",
       "1                  Ridge         mic10  Ret_MinutesFut      2.95934   \n",
       "2                  Ridge         mic20  Ret_MinutesFut      4.24575   \n",
       "3                  Ridge         mic30  Ret_MinutesFut      5.65856   \n",
       "4                  Ridge        corr10  Ret_MinutesFut      2.88769   \n",
       "5                  Ridge        corr20  Ret_MinutesFut       4.3113   \n",
       "6                  Ridge        corr30  Ret_MinutesFut      5.64229   \n",
       "7                  Ridge         rfe10  Ret_MinutesFut      2.90213   \n",
       "8                  Ridge         rfe20  Ret_MinutesFut      4.15243   \n",
       "9                  Ridge         rfe30  Ret_MinutesFut      5.97855   \n",
       "10                 Ridge  polycorr15_1  Ret_MinutesFut      3.58621   \n",
       "11                 Ridge  polycorr15_2  Ret_MinutesFut      3.74265   \n",
       "12                 Ridge     micpoly15  Ret_MinutesFut      3.77019   \n",
       "13                 Ridge          core  Ret_MinutesFut      2.43327   \n",
       "14                 Ridge         mic10  Ret_MinutesFut     0.888162   \n",
       "15                 Ridge         mic20  Ret_MinutesFut         1.25   \n",
       "16                 Ridge         mic30  Ret_MinutesFut      1.89993   \n",
       "17                 Ridge        corr10  Ret_MinutesFut     0.848941   \n",
       "18                 Ridge        corr20  Ret_MinutesFut      1.29687   \n",
       "19                 Ridge        corr30  Ret_MinutesFut      1.65108   \n",
       "20                 Ridge         rfe10  Ret_MinutesFut     0.995591   \n",
       "21                 Ridge         rfe20  Ret_MinutesFut      1.24556   \n",
       "22                 Ridge         rfe30  Ret_MinutesFut       1.6654   \n",
       "23                 Ridge  polycorr15_1  Ret_MinutesFut      1.01822   \n",
       "24                 Ridge  polycorr15_2  Ret_MinutesFut      1.07162   \n",
       "25                 Ridge     micpoly15  Ret_MinutesFut      1.08723   \n",
       "26   ExtraTreesRegressor          core  Ret_MinutesFut      11.5318   \n",
       "27   ExtraTreesRegressor         mic10  Ret_MinutesFut      9.74065   \n",
       "28   ExtraTreesRegressor         mic20  Ret_MinutesFut      9.95673   \n",
       "29   ExtraTreesRegressor         mic30  Ret_MinutesFut      10.1639   \n",
       "..                   ...           ...             ...          ...   \n",
       "80        HuberRegressor  polycorr15_1  Ret_MinutesFut      21.3501   \n",
       "81        HuberRegressor  polycorr15_2  Ret_MinutesFut      27.5333   \n",
       "82        HuberRegressor     micpoly15  Ret_MinutesFut      14.7018   \n",
       "83        HuberRegressor         mic10  Ret_MinutesFut      8.13647   \n",
       "84        HuberRegressor         mic20  Ret_MinutesFut      15.8525   \n",
       "85        HuberRegressor         mic30  Ret_MinutesFut      46.7602   \n",
       "86        HuberRegressor        corr10  Ret_MinutesFut      28.2562   \n",
       "87        HuberRegressor        corr20  Ret_MinutesFut      53.5698   \n",
       "88        HuberRegressor        corr30  Ret_MinutesFut       249.03   \n",
       "89        HuberRegressor  polycorr15_1  Ret_MinutesFut      16.0943   \n",
       "90        HuberRegressor  polycorr15_2  Ret_MinutesFut      18.3686   \n",
       "91        HuberRegressor     micpoly15  Ret_MinutesFut      8.58279   \n",
       "92        HuberRegressor         mic10  Ret_MinutesFut      3.84467   \n",
       "93        HuberRegressor         mic20  Ret_MinutesFut      6.74144   \n",
       "94        HuberRegressor         mic30  Ret_MinutesFut      16.4485   \n",
       "95        HuberRegressor        corr10  Ret_MinutesFut      11.5186   \n",
       "96        HuberRegressor        corr20  Ret_MinutesFut       25.312   \n",
       "97        HuberRegressor        corr30  Ret_MinutesFut      116.644   \n",
       "98        HuberRegressor  polycorr15_1  Ret_MinutesFut       6.9072   \n",
       "99        HuberRegressor  polycorr15_2  Ret_MinutesFut      9.00794   \n",
       "100       HuberRegressor     micpoly15  Ret_MinutesFut      4.39591   \n",
       "101       HuberRegressor         mic10  Ret_MinutesFut      4.08746   \n",
       "102       HuberRegressor         mic20  Ret_MinutesFut      8.13355   \n",
       "103       HuberRegressor         mic30  Ret_MinutesFut      38.9431   \n",
       "104       HuberRegressor        corr10  Ret_MinutesFut      17.5924   \n",
       "105       HuberRegressor        corr20  Ret_MinutesFut      35.7515   \n",
       "106       HuberRegressor        corr30  Ret_MinutesFut       121.18   \n",
       "107       HuberRegressor  polycorr15_1  Ret_MinutesFut      9.35712   \n",
       "108       HuberRegressor  polycorr15_2  Ret_MinutesFut      11.0073   \n",
       "109       HuberRegressor     micpoly15  Ret_MinutesFut      4.07948   \n",
       "\n",
       "    zerobenchmark validationmark         mae  \\\n",
       "0         1773.92        1774.13  0.00485384   \n",
       "1         1773.92        1774.12  0.00485556   \n",
       "2         1773.92         1774.2  0.00486075   \n",
       "3         1773.92        1774.24  0.00485748   \n",
       "4         1773.92        1774.11  0.00485946   \n",
       "5         1773.92        1774.12  0.00486077   \n",
       "6         1773.92        1774.12  0.00486032   \n",
       "7         1773.92         1774.1  0.00485698   \n",
       "8         1773.92        1774.11  0.00485746   \n",
       "9         1773.92        1774.19  0.00486123   \n",
       "10        1773.92        1774.08  0.00485676   \n",
       "11        1773.92        1774.16  0.00486672   \n",
       "12        1773.92        1774.15  0.00486089   \n",
       "13        1773.92        1774.09  0.00485493   \n",
       "14        1773.92        1774.08  0.00485376   \n",
       "15        1773.92        1774.07  0.00485433   \n",
       "16        1773.92         1774.1  0.00485205   \n",
       "17        1773.92        1774.09  0.00485665   \n",
       "18        1773.92        1774.09  0.00485679   \n",
       "19        1773.92        1774.09  0.00485673   \n",
       "20        1773.92        1774.09  0.00485593   \n",
       "21        1773.92        1774.08  0.00485495   \n",
       "22        1773.92        1774.08  0.00485463   \n",
       "23        1773.92        1774.08  0.00485637   \n",
       "24        1773.92        1774.09   0.0048573   \n",
       "25        1773.92        1774.07  0.00485498   \n",
       "26        1773.92            NaN  0.00485626   \n",
       "27        1773.92            NaN  0.00485601   \n",
       "28        1773.92            NaN   0.0048561   \n",
       "29        1773.92            NaN  0.00485511   \n",
       "..            ...            ...         ...   \n",
       "80        1773.92        1774.07  0.00484718   \n",
       "81        1773.92        1774.26  0.00484339   \n",
       "82        1773.92        1773.93  0.00484705   \n",
       "83        1773.92        1773.93  0.00484682   \n",
       "84        1773.92        1774.12  0.00484209   \n",
       "85        1773.92        1774.64  0.00484169   \n",
       "86        1773.92        1774.72  0.00483175   \n",
       "87        1773.92        1774.78  0.00483822   \n",
       "88        1773.92        1774.83   0.0048415   \n",
       "89        1773.92        1774.07  0.00484718   \n",
       "90        1773.92        1774.26  0.00484339   \n",
       "91        1773.92        1773.93  0.00484705   \n",
       "92        1773.92        1773.93  0.00484679   \n",
       "93        1773.92        1774.03  0.00484251   \n",
       "94        1773.92        1774.54  0.00483515   \n",
       "95        1773.92        1774.67  0.00482982   \n",
       "96        1773.92        1774.74  0.00483566   \n",
       "97        1773.92        1774.79  0.00483677   \n",
       "98        1773.92        1774.04  0.00484686   \n",
       "99        1773.92        1774.22  0.00484556   \n",
       "100       1773.92        1773.93  0.00484688   \n",
       "101       1773.92        1773.93  0.00484721   \n",
       "102       1773.92        1774.25  0.00484593   \n",
       "103       1773.92        1774.73  0.00486028   \n",
       "104       1773.92        1774.76  0.00483674   \n",
       "105       1773.92        1774.81  0.00484579   \n",
       "106       1773.92        1774.86  0.00484675   \n",
       "107       1773.92        1774.09  0.00484932   \n",
       "108       1773.92        1774.31   0.0048409   \n",
       "109       1773.92        1773.93  0.00484913   \n",
       "\n",
       "                                                params  \n",
       "0    {'alpha': 6, 'copy_X': True, 'fit_intercept': ...  \n",
       "1    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "2    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "3    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "4    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "5    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "6    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "7    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "8    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "9    {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "10   {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "11   {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "12   {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "13   {'alpha': 20, 'copy_X': True, 'fit_intercept':...  \n",
       "14   {'alpha': 50, 'copy_X': True, 'fit_intercept':...  \n",
       "15   {'alpha': 100, 'copy_X': True, 'fit_intercept'...  \n",
       "16   {'alpha': 50, 'copy_X': True, 'fit_intercept':...  \n",
       "17   {'alpha': 200, 'copy_X': True, 'fit_intercept'...  \n",
       "18   {'alpha': 200, 'copy_X': True, 'fit_intercept'...  \n",
       "19   {'alpha': 200, 'copy_X': True, 'fit_intercept'...  \n",
       "20   {'alpha': 50, 'copy_X': True, 'fit_intercept':...  \n",
       "21   {'alpha': 50, 'copy_X': True, 'fit_intercept':...  \n",
       "22   {'alpha': 100, 'copy_X': True, 'fit_intercept'...  \n",
       "23   {'alpha': 200, 'copy_X': True, 'fit_intercept'...  \n",
       "24   {'alpha': 200, 'copy_X': True, 'fit_intercept'...  \n",
       "25   {'alpha': 100, 'copy_X': True, 'fit_intercept'...  \n",
       "26   {'bootstrap': False, 'criterion': 'mse', 'max_...  \n",
       "27   {'bootstrap': False, 'criterion': 'mse', 'max_...  \n",
       "28   {'bootstrap': False, 'criterion': 'mse', 'max_...  \n",
       "29   {'bootstrap': False, 'criterion': 'mse', 'max_...  \n",
       "..                                                 ...  \n",
       "80   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "81   {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "82   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "83   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "84   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "85   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "86   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "87   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "88   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "89   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "90   {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "91   {'alpha': 200, 'epsilon': 1.35, 'fit_intercept...  \n",
       "92   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "93   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "94   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "95   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "96   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "97   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "98   {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "99   {'alpha': 100, 'epsilon': 1.35, 'fit_intercept...  \n",
       "100  {'alpha': 500, 'epsilon': 1.35, 'fit_intercept...  \n",
       "101  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "102  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "103  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "104  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "105  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "106  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "107  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "108  {'alpha': 5, 'epsilon': 1.35, 'fit_intercept':...  \n",
       "109  {'alpha': 20, 'epsilon': 1.35, 'fit_intercept'...  \n",
       "\n",
       "[110 rows x 8 columns]"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "book = load_workbook('BA_Submissions/Results.xlsx')\n",
    "writer = pd.ExcelWriter('BA_Submissions/Results.xlsx', engine='openpyxl')\n",
    "writer.book = book\n",
    "writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "writer.sheets['Tabelle1']\n",
    "startrow = writer.sheets['Tabelle1'].max_row\n",
    "results.to_excel(writer, index = False,header=False,startrow=startrow,sheet_name ='Tabelle1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
